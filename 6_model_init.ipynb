{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 매개변수 최적화 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터\n",
    "- 에폭(epoch) 수 - 데이터셋을 반복하는 횟수\n",
    "- 배치크기(batch size) - 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    "- 학습률(lr) - 각 배치/애폭에서 모델의 매개변수를 조절하는 비율. 값이 작은 수록 학습속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lerning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 단계\n",
    "- 학습 단계: 학습용 데이터셋을 반복하고 최적의 매개변수로 수렴합니다.\n",
    "- 검증/테스트 단계: 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수\n",
    "학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높습니다. **손실함수**는 획득한 결과와 실제 값 사이의 틀린 정도를 측정하며, 학습 중에 이 값을 최소화하려고합니다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답을 비교하여 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저\n",
    "최적화는 각 학습단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. **최적화 알고리즘**은 이 과정이 수행되는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lerning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch & 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fun):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "loss: 2.294700 [   64/60000]\n",
      "loss: 2.300138 [  128/60000]\n",
      "loss: 2.315917 [  192/60000]\n",
      "loss: 2.309500 [  256/60000]\n",
      "loss: 2.303650 [  576/60000]\n",
      "loss: 2.315490 [  640/60000]\n",
      "loss: 2.310088 [  704/60000]\n",
      "loss: 2.306830 [  768/60000]\n",
      "loss: 2.291355 [ 1088/60000]\n",
      "loss: 2.293030 [ 1152/60000]\n",
      "loss: 2.303245 [ 1216/60000]\n",
      "loss: 2.297059 [ 1280/60000]\n",
      "loss: 2.289589 [ 1600/60000]\n",
      "loss: 2.291426 [ 1664/60000]\n",
      "loss: 2.306644 [ 1728/60000]\n",
      "loss: 2.304031 [ 1792/60000]\n",
      "loss: 2.277407 [ 8256/60000]\n",
      "loss: 2.282192 [ 8320/60000]\n",
      "loss: 2.274353 [ 8384/60000]\n",
      "loss: 2.276133 [ 8448/60000]\n",
      "loss: 2.283121 [ 8768/60000]\n",
      "loss: 2.279510 [ 8832/60000]\n",
      "loss: 2.293397 [ 8896/60000]\n",
      "loss: 2.278743 [ 8960/60000]\n",
      "loss: 2.285091 [ 9280/60000]\n",
      "loss: 2.276168 [ 9344/60000]\n",
      "loss: 2.280649 [ 9408/60000]\n",
      "loss: 2.294257 [ 9472/60000]\n",
      "loss: 2.289640 [ 9792/60000]\n",
      "loss: 2.281523 [ 9856/60000]\n",
      "loss: 2.278614 [ 9920/60000]\n",
      "loss: 2.275810 [ 9984/60000]\n",
      "loss: 2.274391 [16448/60000]\n",
      "loss: 2.253096 [16512/60000]\n",
      "loss: 2.256040 [16576/60000]\n",
      "loss: 2.254692 [16640/60000]\n",
      "loss: 2.261065 [16960/60000]\n",
      "loss: 2.275905 [17024/60000]\n",
      "loss: 2.272748 [17088/60000]\n",
      "loss: 2.261760 [17152/60000]\n",
      "loss: 2.254412 [17472/60000]\n",
      "loss: 2.274223 [17536/60000]\n",
      "loss: 2.250575 [17600/60000]\n",
      "loss: 2.258880 [17664/60000]\n",
      "loss: 2.263895 [17984/60000]\n",
      "loss: 2.268706 [18048/60000]\n",
      "loss: 2.271268 [18112/60000]\n",
      "loss: 2.269458 [18176/60000]\n",
      "loss: 2.240050 [24640/60000]\n",
      "loss: 2.260879 [24704/60000]\n",
      "loss: 2.249424 [24768/60000]\n",
      "loss: 2.245305 [24832/60000]\n",
      "loss: 2.263633 [25152/60000]\n",
      "loss: 2.253131 [25216/60000]\n",
      "loss: 2.244380 [25280/60000]\n",
      "loss: 2.232138 [25344/60000]\n",
      "loss: 2.246133 [25664/60000]\n",
      "loss: 2.247738 [25728/60000]\n",
      "loss: 2.239687 [25792/60000]\n",
      "loss: 2.239846 [25856/60000]\n",
      "loss: 2.255031 [26176/60000]\n",
      "loss: 2.242089 [26240/60000]\n",
      "loss: 2.247280 [26304/60000]\n",
      "loss: 2.242440 [26368/60000]\n",
      "loss: 2.228777 [32832/60000]\n",
      "loss: 2.227215 [32896/60000]\n",
      "loss: 2.222194 [32960/60000]\n",
      "loss: 2.238696 [33024/60000]\n",
      "loss: 2.225588 [33344/60000]\n",
      "loss: 2.208738 [33408/60000]\n",
      "loss: 2.229326 [33472/60000]\n",
      "loss: 2.208617 [33536/60000]\n",
      "loss: 2.228462 [33856/60000]\n",
      "loss: 2.224403 [33920/60000]\n",
      "loss: 2.235499 [33984/60000]\n",
      "loss: 2.217241 [34048/60000]\n",
      "loss: 2.202770 [34368/60000]\n",
      "loss: 2.221221 [34432/60000]\n",
      "loss: 2.231113 [34496/60000]\n",
      "loss: 2.200904 [34560/60000]\n",
      "loss: 2.220595 [41024/60000]\n",
      "loss: 2.204866 [41088/60000]\n",
      "loss: 2.182367 [41152/60000]\n",
      "loss: 2.186674 [41216/60000]\n",
      "loss: 2.212806 [41536/60000]\n",
      "loss: 2.228585 [41600/60000]\n",
      "loss: 2.189975 [41664/60000]\n",
      "loss: 2.204924 [41728/60000]\n",
      "loss: 2.194431 [42048/60000]\n",
      "loss: 2.196432 [42112/60000]\n",
      "loss: 2.198813 [42176/60000]\n",
      "loss: 2.207645 [42240/60000]\n",
      "loss: 2.203077 [42560/60000]\n",
      "loss: 2.180469 [42624/60000]\n",
      "loss: 2.195284 [42688/60000]\n",
      "loss: 2.194649 [42752/60000]\n",
      "loss: 2.189086 [49216/60000]\n",
      "loss: 2.173208 [49280/60000]\n",
      "loss: 2.198028 [49344/60000]\n",
      "loss: 2.151268 [49408/60000]\n",
      "loss: 2.194264 [49728/60000]\n",
      "loss: 2.189489 [49792/60000]\n",
      "loss: 2.158362 [49856/60000]\n",
      "loss: 2.165213 [49920/60000]\n",
      "loss: 2.190410 [50240/60000]\n",
      "loss: 2.212439 [50304/60000]\n",
      "loss: 2.190389 [50368/60000]\n",
      "loss: 2.166145 [50432/60000]\n",
      "loss: 2.181563 [50752/60000]\n",
      "loss: 2.184480 [50816/60000]\n",
      "loss: 2.186021 [50880/60000]\n",
      "loss: 2.194979 [50944/60000]\n",
      "loss: 2.150110 [57408/60000]\n",
      "loss: 2.162734 [57472/60000]\n",
      "loss: 2.174408 [57536/60000]\n",
      "loss: 2.151176 [57600/60000]\n",
      "loss: 2.136524 [57920/60000]\n",
      "loss: 2.140886 [57984/60000]\n",
      "loss: 2.147007 [58048/60000]\n",
      "loss: 2.156133 [58112/60000]\n",
      "loss: 2.171815 [58432/60000]\n",
      "loss: 2.174722 [58496/60000]\n",
      "loss: 2.166210 [58560/60000]\n",
      "loss: 2.157006 [58624/60000]\n",
      "loss: 2.163947 [58944/60000]\n",
      "loss: 2.135381 [59008/60000]\n",
      "loss: 2.153125 [59072/60000]\n",
      "loss: 2.159273 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.151670 \n",
      "\n",
      "Epoch 2\n",
      "\n",
      "loss: 2.151643 [   64/60000]\n",
      "loss: 2.136957 [  128/60000]\n",
      "loss: 2.157778 [  192/60000]\n",
      "loss: 2.166369 [  256/60000]\n",
      "loss: 2.148431 [  576/60000]\n",
      "loss: 2.155298 [  640/60000]\n",
      "loss: 2.147611 [  704/60000]\n",
      "loss: 2.136260 [  768/60000]\n",
      "loss: 2.134861 [ 1088/60000]\n",
      "loss: 2.144285 [ 1152/60000]\n",
      "loss: 2.143838 [ 1216/60000]\n",
      "loss: 2.152042 [ 1280/60000]\n",
      "loss: 2.116838 [ 1600/60000]\n",
      "loss: 2.111669 [ 1664/60000]\n",
      "loss: 2.121029 [ 1728/60000]\n",
      "loss: 2.155450 [ 1792/60000]\n",
      "loss: 2.136341 [ 8256/60000]\n",
      "loss: 2.088071 [ 8320/60000]\n",
      "loss: 2.096128 [ 8384/60000]\n",
      "loss: 2.122174 [ 8448/60000]\n",
      "loss: 2.106420 [ 8768/60000]\n",
      "loss: 2.121356 [ 8832/60000]\n",
      "loss: 2.158132 [ 8896/60000]\n",
      "loss: 2.116744 [ 8960/60000]\n",
      "loss: 2.096620 [ 9280/60000]\n",
      "loss: 2.108441 [ 9344/60000]\n",
      "loss: 2.091486 [ 9408/60000]\n",
      "loss: 2.152462 [ 9472/60000]\n",
      "loss: 2.122472 [ 9792/60000]\n",
      "loss: 2.125960 [ 9856/60000]\n",
      "loss: 2.115377 [ 9920/60000]\n",
      "loss: 2.125138 [ 9984/60000]\n",
      "loss: 2.107026 [16448/60000]\n",
      "loss: 2.049284 [16512/60000]\n",
      "loss: 2.072523 [16576/60000]\n",
      "loss: 2.088644 [16640/60000]\n",
      "loss: 2.087051 [16960/60000]\n",
      "loss: 2.106704 [17024/60000]\n",
      "loss: 2.107682 [17088/60000]\n",
      "loss: 2.094873 [17152/60000]\n",
      "loss: 2.081866 [17472/60000]\n",
      "loss: 2.109434 [17536/60000]\n",
      "loss: 2.061188 [17600/60000]\n",
      "loss: 2.092808 [17664/60000]\n",
      "loss: 2.094184 [17984/60000]\n",
      "loss: 2.103449 [18048/60000]\n",
      "loss: 2.125449 [18112/60000]\n",
      "loss: 2.095696 [18176/60000]\n",
      "loss: 2.019084 [24640/60000]\n",
      "loss: 2.083060 [24704/60000]\n",
      "loss: 2.062389 [24768/60000]\n",
      "loss: 2.063668 [24832/60000]\n",
      "loss: 2.091077 [25152/60000]\n",
      "loss: 2.063422 [25216/60000]\n",
      "loss: 2.034940 [25280/60000]\n",
      "loss: 2.017118 [25344/60000]\n",
      "loss: 2.057800 [25664/60000]\n",
      "loss: 2.053213 [25728/60000]\n",
      "loss: 2.055119 [25792/60000]\n",
      "loss: 2.046073 [25856/60000]\n",
      "loss: 2.078744 [26176/60000]\n",
      "loss: 2.063330 [26240/60000]\n",
      "loss: 2.056118 [26304/60000]\n",
      "loss: 2.046784 [26368/60000]\n",
      "loss: 2.035671 [32832/60000]\n",
      "loss: 2.020812 [32896/60000]\n",
      "loss: 2.013333 [32960/60000]\n",
      "loss: 2.051353 [33024/60000]\n",
      "loss: 2.011914 [33344/60000]\n",
      "loss: 1.991138 [33408/60000]\n",
      "loss: 2.023791 [33472/60000]\n",
      "loss: 1.990336 [33536/60000]\n",
      "loss: 2.023966 [33856/60000]\n",
      "loss: 2.006872 [33920/60000]\n",
      "loss: 2.040116 [33984/60000]\n",
      "loss: 1.994759 [34048/60000]\n",
      "loss: 1.978194 [34368/60000]\n",
      "loss: 2.020586 [34432/60000]\n",
      "loss: 2.020566 [34496/60000]\n",
      "loss: 1.965404 [34560/60000]\n",
      "loss: 2.018048 [41024/60000]\n",
      "loss: 1.994693 [41088/60000]\n",
      "loss: 1.928904 [41152/60000]\n",
      "loss: 1.938723 [41216/60000]\n",
      "loss: 2.005398 [41536/60000]\n",
      "loss: 2.009936 [41600/60000]\n",
      "loss: 1.954852 [41664/60000]\n",
      "loss: 1.971127 [41728/60000]\n",
      "loss: 1.956902 [42048/60000]\n",
      "loss: 1.976305 [42112/60000]\n",
      "loss: 1.951752 [42176/60000]\n",
      "loss: 1.988889 [42240/60000]\n",
      "loss: 1.982131 [42560/60000]\n",
      "loss: 1.900893 [42624/60000]\n",
      "loss: 1.954878 [42688/60000]\n",
      "loss: 1.980841 [42752/60000]\n",
      "loss: 1.959459 [49216/60000]\n",
      "loss: 1.920157 [49280/60000]\n",
      "loss: 1.965050 [49344/60000]\n",
      "loss: 1.869549 [49408/60000]\n",
      "loss: 1.950720 [49728/60000]\n",
      "loss: 1.944244 [49792/60000]\n",
      "loss: 1.862164 [49856/60000]\n",
      "loss: 1.920857 [49920/60000]\n",
      "loss: 1.950318 [50240/60000]\n",
      "loss: 1.994711 [50304/60000]\n",
      "loss: 1.958910 [50368/60000]\n",
      "loss: 1.895948 [50432/60000]\n",
      "loss: 1.928679 [50752/60000]\n",
      "loss: 1.936264 [50816/60000]\n",
      "loss: 1.935021 [50880/60000]\n",
      "loss: 1.949376 [50944/60000]\n",
      "loss: 1.872903 [57408/60000]\n",
      "loss: 1.876254 [57472/60000]\n",
      "loss: 1.912263 [57536/60000]\n",
      "loss: 1.875724 [57600/60000]\n",
      "loss: 1.845065 [57920/60000]\n",
      "loss: 1.836773 [57984/60000]\n",
      "loss: 1.873595 [58048/60000]\n",
      "loss: 1.894043 [58112/60000]\n",
      "loss: 1.918585 [58432/60000]\n",
      "loss: 1.924969 [58496/60000]\n",
      "loss: 1.904703 [58560/60000]\n",
      "loss: 1.888119 [58624/60000]\n",
      "loss: 1.908872 [58944/60000]\n",
      "loss: 1.840563 [59008/60000]\n",
      "loss: 1.869717 [59072/60000]\n",
      "loss: 1.887321 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 1.875605 \n",
      "\n",
      "Epoch 3\n",
      "\n",
      "loss: 1.898567 [   64/60000]\n",
      "loss: 1.821844 [  128/60000]\n",
      "loss: 1.872172 [  192/60000]\n",
      "loss: 1.924580 [  256/60000]\n",
      "loss: 1.854915 [  576/60000]\n",
      "loss: 1.874633 [  640/60000]\n",
      "loss: 1.837354 [  704/60000]\n",
      "loss: 1.822773 [  768/60000]\n",
      "loss: 1.846404 [ 1088/60000]\n",
      "loss: 1.878042 [ 1152/60000]\n",
      "loss: 1.859150 [ 1216/60000]\n",
      "loss: 1.904236 [ 1280/60000]\n",
      "loss: 1.789857 [ 1600/60000]\n",
      "loss: 1.804416 [ 1664/60000]\n",
      "loss: 1.787196 [ 1728/60000]\n",
      "loss: 1.879100 [ 1792/60000]\n",
      "loss: 1.874132 [ 8256/60000]\n",
      "loss: 1.752247 [ 8320/60000]\n",
      "loss: 1.794973 [ 8384/60000]\n",
      "loss: 1.843589 [ 8448/60000]\n",
      "loss: 1.780493 [ 8768/60000]\n",
      "loss: 1.822294 [ 8832/60000]\n",
      "loss: 1.893117 [ 8896/60000]\n",
      "loss: 1.829079 [ 8960/60000]\n",
      "loss: 1.765822 [ 9280/60000]\n",
      "loss: 1.781609 [ 9344/60000]\n",
      "loss: 1.766160 [ 9408/60000]\n",
      "loss: 1.871987 [ 9472/60000]\n",
      "loss: 1.811007 [ 9792/60000]\n",
      "loss: 1.853516 [ 9856/60000]\n",
      "loss: 1.802132 [ 9920/60000]\n",
      "loss: 1.846176 [ 9984/60000]\n",
      "loss: 1.778939 [16448/60000]\n",
      "loss: 1.689125 [16512/60000]\n",
      "loss: 1.723310 [16576/60000]\n",
      "loss: 1.772457 [16640/60000]\n",
      "loss: 1.764736 [16960/60000]\n",
      "loss: 1.801980 [17024/60000]\n",
      "loss: 1.796556 [17088/60000]\n",
      "loss: 1.807611 [17152/60000]\n",
      "loss: 1.771506 [17472/60000]\n",
      "loss: 1.803693 [17536/60000]\n",
      "loss: 1.721853 [17600/60000]\n",
      "loss: 1.784676 [17664/60000]\n",
      "loss: 1.785976 [17984/60000]\n",
      "loss: 1.802577 [18048/60000]\n",
      "loss: 1.853725 [18112/60000]\n",
      "loss: 1.752585 [18176/60000]\n",
      "loss: 1.644277 [24640/60000]\n",
      "loss: 1.754878 [24704/60000]\n",
      "loss: 1.732749 [24768/60000]\n",
      "loss: 1.752313 [24832/60000]\n",
      "loss: 1.773944 [25152/60000]\n",
      "loss: 1.720965 [25216/60000]\n",
      "loss: 1.649652 [25280/60000]\n",
      "loss: 1.641318 [25344/60000]\n",
      "loss: 1.697651 [25664/60000]\n",
      "loss: 1.713985 [25728/60000]\n",
      "loss: 1.721125 [25792/60000]\n",
      "loss: 1.701767 [25856/60000]\n",
      "loss: 1.745493 [26176/60000]\n",
      "loss: 1.747673 [26240/60000]\n",
      "loss: 1.709655 [26304/60000]\n",
      "loss: 1.679024 [26368/60000]\n",
      "loss: 1.708778 [32832/60000]\n",
      "loss: 1.660910 [32896/60000]\n",
      "loss: 1.659921 [32960/60000]\n",
      "loss: 1.720804 [33024/60000]\n",
      "loss: 1.626261 [33344/60000]\n",
      "loss: 1.604502 [33408/60000]\n",
      "loss: 1.658243 [33472/60000]\n",
      "loss: 1.634343 [33536/60000]\n",
      "loss: 1.660854 [33856/60000]\n",
      "loss: 1.623559 [33920/60000]\n",
      "loss: 1.704774 [33984/60000]\n",
      "loss: 1.623063 [34048/60000]\n",
      "loss: 1.599322 [34368/60000]\n",
      "loss: 1.691239 [34432/60000]\n",
      "loss: 1.640632 [34496/60000]\n",
      "loss: 1.561094 [34560/60000]\n",
      "loss: 1.673545 [41024/60000]\n",
      "loss: 1.637568 [41088/60000]\n",
      "loss: 1.512070 [41152/60000]\n",
      "loss: 1.548564 [41216/60000]\n",
      "loss: 1.677063 [41536/60000]\n",
      "loss: 1.639189 [41600/60000]\n",
      "loss: 1.586809 [41664/60000]\n",
      "loss: 1.599988 [41728/60000]\n",
      "loss: 1.589854 [42048/60000]\n",
      "loss: 1.626906 [42112/60000]\n",
      "loss: 1.544206 [42176/60000]\n",
      "loss: 1.631138 [42240/60000]\n",
      "loss: 1.632258 [42560/60000]\n",
      "loss: 1.460765 [42624/60000]\n",
      "loss: 1.558336 [42688/60000]\n",
      "loss: 1.636687 [42752/60000]\n",
      "loss: 1.601521 [49216/60000]\n",
      "loss: 1.533810 [49280/60000]\n",
      "loss: 1.601190 [49344/60000]\n",
      "loss: 1.448531 [49408/60000]\n",
      "loss: 1.562962 [49728/60000]\n",
      "loss: 1.571906 [49792/60000]\n",
      "loss: 1.428148 [49856/60000]\n",
      "loss: 1.558485 [49920/60000]\n",
      "loss: 1.575251 [50240/60000]\n",
      "loss: 1.659742 [50304/60000]\n",
      "loss: 1.599207 [50368/60000]\n",
      "loss: 1.506880 [50432/60000]\n",
      "loss: 1.539819 [50752/60000]\n",
      "loss: 1.544257 [50816/60000]\n",
      "loss: 1.542241 [50880/60000]\n",
      "loss: 1.572103 [50944/60000]\n",
      "loss: 1.498238 [57408/60000]\n",
      "loss: 1.461135 [57472/60000]\n",
      "loss: 1.559368 [57536/60000]\n",
      "loss: 1.508358 [57600/60000]\n",
      "loss: 1.464767 [57920/60000]\n",
      "loss: 1.421710 [57984/60000]\n",
      "loss: 1.499676 [58048/60000]\n",
      "loss: 1.531047 [58112/60000]\n",
      "loss: 1.583809 [58432/60000]\n",
      "loss: 1.567278 [58496/60000]\n",
      "loss: 1.512642 [58560/60000]\n",
      "loss: 1.518143 [58624/60000]\n",
      "loss: 1.568027 [58944/60000]\n",
      "loss: 1.433821 [59008/60000]\n",
      "loss: 1.472224 [59072/60000]\n",
      "loss: 1.508128 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.505611 \n",
      "\n",
      "Epoch 4\n",
      "\n",
      "loss: 1.560631 [   64/60000]\n",
      "loss: 1.401832 [  128/60000]\n",
      "loss: 1.482195 [  192/60000]\n",
      "loss: 1.593576 [  256/60000]\n",
      "loss: 1.473600 [  576/60000]\n",
      "loss: 1.493320 [  640/60000]\n",
      "loss: 1.418129 [  704/60000]\n",
      "loss: 1.407997 [  768/60000]\n",
      "loss: 1.480000 [ 1088/60000]\n",
      "loss: 1.528384 [ 1152/60000]\n",
      "loss: 1.475765 [ 1216/60000]\n",
      "loss: 1.579919 [ 1280/60000]\n",
      "loss: 1.368483 [ 1600/60000]\n",
      "loss: 1.418779 [ 1664/60000]\n",
      "loss: 1.368688 [ 1728/60000]\n",
      "loss: 1.503461 [ 1792/60000]\n",
      "loss: 1.526480 [ 8256/60000]\n",
      "loss: 1.368919 [ 8320/60000]\n",
      "loss: 1.447749 [ 8384/60000]\n",
      "loss: 1.498603 [ 8448/60000]\n",
      "loss: 1.398093 [ 8768/60000]\n",
      "loss: 1.464364 [ 8832/60000]\n",
      "loss: 1.537814 [ 8896/60000]\n",
      "loss: 1.510113 [ 8960/60000]\n",
      "loss: 1.382094 [ 9280/60000]\n",
      "loss: 1.386569 [ 9344/60000]\n",
      "loss: 1.391858 [ 9408/60000]\n",
      "loss: 1.507700 [ 9472/60000]\n",
      "loss: 1.449816 [ 9792/60000]\n",
      "loss: 1.528368 [ 9856/60000]\n",
      "loss: 1.432764 [ 9920/60000]\n",
      "loss: 1.516921 [ 9984/60000]\n",
      "loss: 1.390187 [16448/60000]\n",
      "loss: 1.299337 [16512/60000]\n",
      "loss: 1.372019 [16576/60000]\n",
      "loss: 1.439399 [16640/60000]\n",
      "loss: 1.407258 [16960/60000]\n",
      "loss: 1.454687 [17024/60000]\n",
      "loss: 1.441237 [17088/60000]\n",
      "loss: 1.502814 [17152/60000]\n",
      "loss: 1.436918 [17472/60000]\n",
      "loss: 1.488189 [17536/60000]\n",
      "loss: 1.360657 [17600/60000]\n",
      "loss: 1.450511 [17664/60000]\n",
      "loss: 1.448224 [17984/60000]\n",
      "loss: 1.470584 [18048/60000]\n",
      "loss: 1.563990 [18112/60000]\n",
      "loss: 1.363563 [18176/60000]\n",
      "loss: 1.291937 [24640/60000]\n",
      "loss: 1.427171 [24704/60000]\n",
      "loss: 1.406396 [24768/60000]\n",
      "loss: 1.456697 [24832/60000]\n",
      "loss: 1.442292 [25152/60000]\n",
      "loss: 1.382554 [25216/60000]\n",
      "loss: 1.274463 [25280/60000]\n",
      "loss: 1.295014 [25344/60000]\n",
      "loss: 1.352286 [25664/60000]\n",
      "loss: 1.366054 [25728/60000]\n",
      "loss: 1.397675 [25792/60000]\n",
      "loss: 1.357582 [25856/60000]\n",
      "loss: 1.407465 [26176/60000]\n",
      "loss: 1.477824 [26240/60000]\n",
      "loss: 1.376332 [26304/60000]\n",
      "loss: 1.309043 [26368/60000]\n",
      "loss: 1.386578 [32832/60000]\n",
      "loss: 1.335682 [32896/60000]\n",
      "loss: 1.358794 [32960/60000]\n",
      "loss: 1.433086 [33024/60000]\n",
      "loss: 1.276803 [33344/60000]\n",
      "loss: 1.281523 [33408/60000]\n",
      "loss: 1.317679 [33472/60000]\n",
      "loss: 1.340630 [33536/60000]\n",
      "loss: 1.340693 [33856/60000]\n",
      "loss: 1.298743 [33920/60000]\n",
      "loss: 1.404316 [33984/60000]\n",
      "loss: 1.297369 [34048/60000]\n",
      "loss: 1.297137 [34368/60000]\n",
      "loss: 1.403089 [34432/60000]\n",
      "loss: 1.284124 [34496/60000]\n",
      "loss: 1.236866 [34560/60000]\n",
      "loss: 1.372592 [41024/60000]\n",
      "loss: 1.329596 [41088/60000]\n",
      "loss: 1.176445 [41152/60000]\n",
      "loss: 1.247483 [41216/60000]\n",
      "loss: 1.406909 [41536/60000]\n",
      "loss: 1.312742 [41600/60000]\n",
      "loss: 1.295306 [41664/60000]\n",
      "loss: 1.311669 [41728/60000]\n",
      "loss: 1.302619 [42048/60000]\n",
      "loss: 1.346277 [42112/60000]\n",
      "loss: 1.212511 [42176/60000]\n",
      "loss: 1.323465 [42240/60000]\n",
      "loss: 1.341410 [42560/60000]\n",
      "loss: 1.126943 [42624/60000]\n",
      "loss: 1.246869 [42688/60000]\n",
      "loss: 1.377501 [42752/60000]\n",
      "loss: 1.301423 [49216/60000]\n",
      "loss: 1.232390 [49280/60000]\n",
      "loss: 1.313831 [49344/60000]\n",
      "loss: 1.146576 [49408/60000]\n",
      "loss: 1.251652 [49728/60000]\n",
      "loss: 1.279101 [49792/60000]\n",
      "loss: 1.112250 [49856/60000]\n",
      "loss: 1.287410 [49920/60000]\n",
      "loss: 1.278757 [50240/60000]\n",
      "loss: 1.398240 [50304/60000]\n",
      "loss: 1.300343 [50368/60000]\n",
      "loss: 1.220548 [50432/60000]\n",
      "loss: 1.241615 [50752/60000]\n",
      "loss: 1.239080 [50816/60000]\n",
      "loss: 1.251053 [50880/60000]\n",
      "loss: 1.282403 [50944/60000]\n",
      "loss: 1.229779 [57408/60000]\n",
      "loss: 1.145933 [57472/60000]\n",
      "loss: 1.299469 [57536/60000]\n",
      "loss: 1.242570 [57600/60000]\n",
      "loss: 1.194077 [57920/60000]\n",
      "loss: 1.125544 [57984/60000]\n",
      "loss: 1.222903 [58048/60000]\n",
      "loss: 1.275139 [58112/60000]\n",
      "loss: 1.360025 [58432/60000]\n",
      "loss: 1.294421 [58496/60000]\n",
      "loss: 1.203021 [58560/60000]\n",
      "loss: 1.242979 [58624/60000]\n",
      "loss: 1.324403 [58944/60000]\n",
      "loss: 1.145197 [59008/60000]\n",
      "loss: 1.186766 [59072/60000]\n",
      "loss: 1.226995 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.241806 \n",
      "\n",
      "Epoch 5\n",
      "\n",
      "loss: 1.306521 [   64/60000]\n",
      "loss: 1.119730 [  128/60000]\n",
      "loss: 1.207155 [  192/60000]\n",
      "loss: 1.336707 [  256/60000]\n",
      "loss: 1.232799 [  576/60000]\n",
      "loss: 1.230855 [  640/60000]\n",
      "loss: 1.117875 [  704/60000]\n",
      "loss: 1.112177 [  768/60000]\n",
      "loss: 1.236246 [ 1088/60000]\n",
      "loss: 1.293379 [ 1152/60000]\n",
      "loss: 1.207624 [ 1216/60000]\n",
      "loss: 1.344942 [ 1280/60000]\n",
      "loss: 1.088283 [ 1600/60000]\n",
      "loss: 1.149846 [ 1664/60000]\n",
      "loss: 1.105855 [ 1728/60000]\n",
      "loss: 1.233008 [ 1792/60000]\n",
      "loss: 1.262628 [ 8256/60000]\n",
      "loss: 1.116900 [ 8320/60000]\n",
      "loss: 1.217804 [ 8384/60000]\n",
      "loss: 1.252591 [ 8448/60000]\n",
      "loss: 1.148855 [ 8768/60000]\n",
      "loss: 1.212656 [ 8832/60000]\n",
      "loss: 1.268574 [ 8896/60000]\n",
      "loss: 1.314080 [ 8960/60000]\n",
      "loss: 1.128030 [ 9280/60000]\n",
      "loss: 1.112098 [ 9344/60000]\n",
      "loss: 1.147518 [ 9408/60000]\n",
      "loss: 1.248973 [ 9472/60000]\n",
      "loss: 1.208335 [ 9792/60000]\n",
      "loss: 1.303583 [ 9856/60000]\n",
      "loss: 1.175036 [ 9920/60000]\n",
      "loss: 1.287768 [ 9984/60000]\n",
      "loss: 1.122275 [16448/60000]\n",
      "loss: 1.043619 [16512/60000]\n",
      "loss: 1.161201 [16576/60000]\n",
      "loss: 1.226514 [16640/60000]\n",
      "loss: 1.159068 [16960/60000]\n",
      "loss: 1.216375 [17024/60000]\n",
      "loss: 1.188202 [17088/60000]\n",
      "loss: 1.285981 [17152/60000]\n",
      "loss: 1.226600 [17472/60000]\n",
      "loss: 1.293662 [17536/60000]\n",
      "loss: 1.111928 [17600/60000]\n",
      "loss: 1.234325 [17664/60000]\n",
      "loss: 1.213527 [17984/60000]\n",
      "loss: 1.251068 [18048/60000]\n",
      "loss: 1.370247 [18112/60000]\n",
      "loss: 1.114487 [18176/60000]\n",
      "loss: 1.081876 [24640/60000]\n",
      "loss: 1.212435 [24704/60000]\n",
      "loss: 1.186168 [24768/60000]\n",
      "loss: 1.271728 [24832/60000]\n",
      "loss: 1.213438 [25152/60000]\n",
      "loss: 1.143569 [25216/60000]\n",
      "loss: 1.033752 [25280/60000]\n",
      "loss: 1.079293 [25344/60000]\n",
      "loss: 1.123923 [25664/60000]\n",
      "loss: 1.127147 [25728/60000]\n",
      "loss: 1.182509 [25792/60000]\n",
      "loss: 1.123613 [25856/60000]\n",
      "loss: 1.187704 [26176/60000]\n",
      "loss: 1.323597 [26240/60000]\n",
      "loss: 1.169594 [26304/60000]\n",
      "loss: 1.076494 [26368/60000]\n",
      "loss: 1.168776 [32832/60000]\n",
      "loss: 1.135501 [32896/60000]\n",
      "loss: 1.164230 [32960/60000]\n",
      "loss: 1.259574 [33024/60000]\n",
      "loss: 1.049080 [33344/60000]\n",
      "loss: 1.083438 [33408/60000]\n",
      "loss: 1.093680 [33472/60000]\n",
      "loss: 1.170321 [33536/60000]\n",
      "loss: 1.132295 [33856/60000]\n",
      "loss: 1.099228 [33920/60000]\n",
      "loss: 1.219775 [33984/60000]\n",
      "loss: 1.093179 [34048/60000]\n",
      "loss: 1.108852 [34368/60000]\n",
      "loss: 1.216375 [34432/60000]\n",
      "loss: 1.043456 [34496/60000]\n",
      "loss: 1.045008 [34560/60000]\n",
      "loss: 1.170946 [41024/60000]\n",
      "loss: 1.129749 [41088/60000]\n",
      "loss: 0.968012 [41152/60000]\n",
      "loss: 1.057885 [41216/60000]\n",
      "loss: 1.231306 [41536/60000]\n",
      "loss: 1.101656 [41600/60000]\n",
      "loss: 1.111343 [41664/60000]\n",
      "loss: 1.138641 [41728/60000]\n",
      "loss: 1.125146 [42048/60000]\n",
      "loss: 1.172810 [42112/60000]\n",
      "loss: 1.009865 [42176/60000]\n",
      "loss: 1.115579 [42240/60000]\n",
      "loss: 1.154261 [42560/60000]\n",
      "loss: 0.920296 [42624/60000]\n",
      "loss: 1.065152 [42688/60000]\n",
      "loss: 1.219838 [42752/60000]\n",
      "loss: 1.098472 [49216/60000]\n",
      "loss: 1.046631 [49280/60000]\n",
      "loss: 1.136825 [49344/60000]\n",
      "loss: 0.964661 [49408/60000]\n",
      "loss: 1.062609 [49728/60000]\n",
      "loss: 1.093783 [49792/60000]\n",
      "loss: 0.919122 [49856/60000]\n",
      "loss: 1.115112 [49920/60000]\n",
      "loss: 1.090752 [50240/60000]\n",
      "loss: 1.238934 [50304/60000]\n",
      "loss: 1.114904 [50368/60000]\n",
      "loss: 1.035817 [50432/60000]\n",
      "loss: 1.055339 [50752/60000]\n",
      "loss: 1.051113 [50816/60000]\n",
      "loss: 1.063920 [50880/60000]\n",
      "loss: 1.104833 [50944/60000]\n",
      "loss: 1.056678 [57408/60000]\n",
      "loss: 0.946936 [57472/60000]\n",
      "loss: 1.133029 [57536/60000]\n",
      "loss: 1.072801 [57600/60000]\n",
      "loss: 1.021696 [57920/60000]\n",
      "loss: 0.933454 [57984/60000]\n",
      "loss: 1.049301 [58048/60000]\n",
      "loss: 1.123988 [58112/60000]\n",
      "loss: 1.224165 [58432/60000]\n",
      "loss: 1.122071 [58496/60000]\n",
      "loss: 1.002398 [58560/60000]\n",
      "loss: 1.064988 [58624/60000]\n",
      "loss: 1.170125 [58944/60000]\n",
      "loss: 0.962294 [59008/60000]\n",
      "loss: 1.014009 [59072/60000]\n",
      "loss: 1.048723 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.076468 \n",
      "\n",
      "Epoch 6\n",
      "\n",
      "loss: 1.136321 [   64/60000]\n",
      "loss: 0.943088 [  128/60000]\n",
      "loss: 1.046512 [  192/60000]\n",
      "loss: 1.175938 [  256/60000]\n",
      "loss: 1.098244 [  576/60000]\n",
      "loss: 1.082126 [  640/60000]\n",
      "loss: 0.924047 [  704/60000]\n",
      "loss: 0.921895 [  768/60000]\n",
      "loss: 1.084120 [ 1088/60000]\n",
      "loss: 1.154394 [ 1152/60000]\n",
      "loss: 1.046023 [ 1216/60000]\n",
      "loss: 1.192614 [ 1280/60000]\n",
      "loss: 0.910097 [ 1600/60000]\n",
      "loss: 0.975578 [ 1664/60000]\n",
      "loss: 0.948391 [ 1728/60000]\n",
      "loss: 1.064993 [ 1792/60000]\n",
      "loss: 1.089022 [ 8256/60000]\n",
      "loss: 0.957167 [ 8320/60000]\n",
      "loss: 1.074254 [ 8384/60000]\n",
      "loss: 1.092006 [ 8448/60000]\n",
      "loss: 0.996960 [ 8768/60000]\n",
      "loss: 1.048688 [ 8832/60000]\n",
      "loss: 1.095873 [ 8896/60000]\n",
      "loss: 1.194137 [ 8960/60000]\n",
      "loss: 0.971937 [ 9280/60000]\n",
      "loss: 0.934451 [ 9344/60000]\n",
      "loss: 1.002271 [ 9408/60000]\n",
      "loss: 1.092945 [ 9472/60000]\n",
      "loss: 1.059120 [ 9792/60000]\n",
      "loss: 1.163021 [ 9856/60000]\n",
      "loss: 1.009309 [ 9920/60000]\n",
      "loss: 1.140252 [ 9984/60000]\n",
      "loss: 0.952034 [16448/60000]\n",
      "loss: 0.885045 [16512/60000]\n",
      "loss: 1.037129 [16576/60000]\n",
      "loss: 1.096161 [16640/60000]\n",
      "loss: 0.996066 [16960/60000]\n",
      "loss: 1.064694 [17024/60000]\n",
      "loss: 1.024034 [17088/60000]\n",
      "loss: 1.139179 [17152/60000]\n",
      "loss: 1.102799 [17472/60000]\n",
      "loss: 1.173947 [17536/60000]\n",
      "loss: 0.948672 [17600/60000]\n",
      "loss: 1.099495 [17664/60000]\n",
      "loss: 1.059867 [17984/60000]\n",
      "loss: 1.122523 [18048/60000]\n",
      "loss: 1.248749 [18112/60000]\n",
      "loss: 0.958597 [18176/60000]\n",
      "loss: 0.957603 [24640/60000]\n",
      "loss: 1.078729 [24704/60000]\n",
      "loss: 1.036579 [24768/60000]\n",
      "loss: 1.156425 [24832/60000]\n",
      "loss: 1.065123 [25152/60000]\n",
      "loss: 0.979213 [25216/60000]\n",
      "loss: 0.889216 [25280/60000]\n",
      "loss: 0.944267 [25344/60000]\n",
      "loss: 0.981871 [25664/60000]\n",
      "loss: 0.973784 [25728/60000]\n",
      "loss: 1.046157 [25792/60000]\n",
      "loss: 0.968357 [25856/60000]\n",
      "loss: 1.051334 [26176/60000]\n",
      "loss: 1.231416 [26240/60000]\n",
      "loss: 1.048345 [26304/60000]\n",
      "loss: 0.929832 [26368/60000]\n",
      "loss: 1.028573 [32832/60000]\n",
      "loss: 1.012892 [32896/60000]\n",
      "loss: 1.039010 [32960/60000]\n",
      "loss: 1.153972 [33024/60000]\n",
      "loss: 0.901788 [33344/60000]\n",
      "loss: 0.963163 [33408/60000]\n",
      "loss: 0.951555 [33472/60000]\n",
      "loss: 1.068387 [33536/60000]\n",
      "loss: 0.999081 [33856/60000]\n",
      "loss: 0.965607 [33920/60000]\n",
      "loss: 1.105403 [33984/60000]\n",
      "loss: 0.962129 [34048/60000]\n",
      "loss: 0.984366 [34368/60000]\n",
      "loss: 1.095320 [34432/60000]\n",
      "loss: 0.883860 [34496/60000]\n",
      "loss: 0.924573 [34560/60000]\n",
      "loss: 1.039350 [41024/60000]\n",
      "loss: 0.995928 [41088/60000]\n",
      "loss: 0.837999 [41152/60000]\n",
      "loss: 0.939959 [41216/60000]\n",
      "loss: 1.112603 [41536/60000]\n",
      "loss: 0.963264 [41600/60000]\n",
      "loss: 0.993304 [41664/60000]\n",
      "loss: 1.026899 [41728/60000]\n",
      "loss: 1.010039 [42048/60000]\n",
      "loss: 1.062590 [42112/60000]\n",
      "loss: 0.882390 [42176/60000]\n",
      "loss: 0.969975 [42240/60000]\n",
      "loss: 1.032144 [42560/60000]\n",
      "loss: 0.787212 [42624/60000]\n",
      "loss: 0.955789 [42688/60000]\n",
      "loss: 1.117758 [42752/60000]\n",
      "loss: 0.960041 [49216/60000]\n",
      "loss: 0.931024 [49280/60000]\n",
      "loss: 1.028485 [49344/60000]\n",
      "loss: 0.850351 [49408/60000]\n",
      "loss: 0.950492 [49728/60000]\n",
      "loss: 0.974041 [49792/60000]\n",
      "loss: 0.793971 [49856/60000]\n",
      "loss: 1.002840 [49920/60000]\n",
      "loss: 0.968838 [50240/60000]\n",
      "loss: 1.139721 [50304/60000]\n",
      "loss: 0.999844 [50368/60000]\n",
      "loss: 0.914091 [50432/60000]\n",
      "loss: 0.933953 [50752/60000]\n",
      "loss: 0.930598 [50816/60000]\n",
      "loss: 0.942739 [50880/60000]\n",
      "loss: 0.990944 [50944/60000]\n",
      "loss: 0.944793 [57408/60000]\n",
      "loss: 0.818461 [57472/60000]\n",
      "loss: 1.026434 [57536/60000]\n",
      "loss: 0.960603 [57600/60000]\n",
      "loss: 0.910854 [57920/60000]\n",
      "loss: 0.808059 [57984/60000]\n",
      "loss: 0.939581 [58048/60000]\n",
      "loss: 1.030820 [58112/60000]\n",
      "loss: 1.137786 [58432/60000]\n",
      "loss: 1.011803 [58496/60000]\n",
      "loss: 0.871323 [58560/60000]\n",
      "loss: 0.950649 [58624/60000]\n",
      "loss: 1.067319 [58944/60000]\n",
      "loss: 0.842858 [59008/60000]\n",
      "loss: 0.908718 [59072/60000]\n",
      "loss: 0.937406 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.970491 \n",
      "\n",
      "Epoch 7\n",
      "\n",
      "loss: 1.019489 [   64/60000]\n",
      "loss: 0.828860 [  128/60000]\n",
      "loss: 0.951336 [  192/60000]\n",
      "loss: 1.073660 [  256/60000]\n",
      "loss: 1.019105 [  576/60000]\n",
      "loss: 0.997075 [  640/60000]\n",
      "loss: 0.800784 [  704/60000]\n",
      "loss: 0.799890 [  768/60000]\n",
      "loss: 0.986841 [ 1088/60000]\n",
      "loss: 1.069448 [ 1152/60000]\n",
      "loss: 0.944092 [ 1216/60000]\n",
      "loss: 1.088869 [ 1280/60000]\n",
      "loss: 0.792737 [ 1600/60000]\n",
      "loss: 0.861099 [ 1664/60000]\n",
      "loss: 0.846997 [ 1728/60000]\n",
      "loss: 0.958381 [ 1792/60000]\n",
      "loss: 0.973845 [ 8256/60000]\n",
      "loss: 0.853180 [ 8320/60000]\n",
      "loss: 0.984565 [ 8384/60000]\n",
      "loss: 0.987687 [ 8448/60000]\n",
      "loss: 0.899690 [ 8768/60000]\n",
      "loss: 0.939526 [ 8832/60000]\n",
      "loss: 0.985082 [ 8896/60000]\n",
      "loss: 1.115318 [ 8960/60000]\n",
      "loss: 0.872294 [ 9280/60000]\n",
      "loss: 0.816984 [ 9344/60000]\n",
      "loss: 0.912490 [ 9408/60000]\n",
      "loss: 0.997272 [ 9472/60000]\n",
      "loss: 0.967300 [ 9792/60000]\n",
      "loss: 1.072006 [ 9856/60000]\n",
      "loss: 0.901321 [ 9920/60000]\n",
      "loss: 1.041449 [ 9984/60000]\n",
      "loss: 0.839995 [16448/60000]\n",
      "loss: 0.783285 [16512/60000]\n",
      "loss: 0.961883 [16576/60000]\n",
      "loss: 1.010723 [16640/60000]\n",
      "loss: 0.888890 [16960/60000]\n",
      "loss: 0.967055 [17024/60000]\n",
      "loss: 0.917450 [17088/60000]\n",
      "loss: 1.038002 [17152/60000]\n",
      "loss: 1.025016 [17472/60000]\n",
      "loss: 1.093466 [17536/60000]\n",
      "loss: 0.841418 [17600/60000]\n",
      "loss: 1.013499 [17664/60000]\n",
      "loss: 0.957442 [17984/60000]\n",
      "loss: 1.046207 [18048/60000]\n",
      "loss: 1.167409 [18112/60000]\n",
      "loss: 0.855964 [18176/60000]\n",
      "loss: 0.880402 [24640/60000]\n",
      "loss: 0.994420 [24704/60000]\n",
      "loss: 0.931022 [24768/60000]\n",
      "loss: 1.079712 [24832/60000]\n",
      "loss: 0.966312 [25152/60000]\n",
      "loss: 0.866570 [25216/60000]\n",
      "loss: 0.801551 [25280/60000]\n",
      "loss: 0.856710 [25344/60000]\n",
      "loss: 0.892990 [25664/60000]\n",
      "loss: 0.873317 [25728/60000]\n",
      "loss: 0.959122 [25792/60000]\n",
      "loss: 0.860954 [25856/60000]\n",
      "loss: 0.964967 [26176/60000]\n",
      "loss: 1.171845 [26240/60000]\n",
      "loss: 0.975877 [26304/60000]\n",
      "loss: 0.833975 [26368/60000]\n",
      "loss: 0.937218 [32832/60000]\n",
      "loss: 0.934323 [32896/60000]\n",
      "loss: 0.956072 [32960/60000]\n",
      "loss: 1.081907 [33024/60000]\n",
      "loss: 0.802699 [33344/60000]\n",
      "loss: 0.891305 [33408/60000]\n",
      "loss: 0.861422 [33472/60000]\n",
      "loss: 1.004829 [33536/60000]\n",
      "loss: 0.911394 [33856/60000]\n",
      "loss: 0.869740 [33920/60000]\n",
      "loss: 1.032418 [33984/60000]\n",
      "loss: 0.876026 [34048/60000]\n",
      "loss: 0.897559 [34368/60000]\n",
      "loss: 1.013568 [34432/60000]\n",
      "loss: 0.777548 [34496/60000]\n",
      "loss: 0.844914 [34560/60000]\n",
      "loss: 0.953243 [41024/60000]\n",
      "loss: 0.904574 [41088/60000]\n",
      "loss: 0.754540 [41152/60000]\n",
      "loss: 0.865999 [41216/60000]\n",
      "loss: 1.029857 [41536/60000]\n",
      "loss: 0.867630 [41600/60000]\n",
      "loss: 0.917051 [41664/60000]\n",
      "loss: 0.950493 [41728/60000]\n",
      "loss: 0.932386 [42048/60000]\n",
      "loss: 0.987833 [42112/60000]\n",
      "loss: 0.796930 [42176/60000]\n",
      "loss: 0.865758 [42240/60000]\n",
      "loss: 0.950288 [42560/60000]\n",
      "loss: 0.698930 [42624/60000]\n",
      "loss: 0.884865 [42688/60000]\n",
      "loss: 1.046649 [42752/60000]\n",
      "loss: 0.861987 [49216/60000]\n",
      "loss: 0.856478 [49280/60000]\n",
      "loss: 0.959160 [49344/60000]\n",
      "loss: 0.775685 [49408/60000]\n",
      "loss: 0.882294 [49728/60000]\n",
      "loss: 0.893384 [49792/60000]\n",
      "loss: 0.708416 [49856/60000]\n",
      "loss: 0.927215 [49920/60000]\n",
      "loss: 0.887499 [50240/60000]\n",
      "loss: 1.073827 [50304/60000]\n",
      "loss: 0.924872 [50368/60000]\n",
      "loss: 0.830761 [50432/60000]\n",
      "loss: 0.850300 [50752/60000]\n",
      "loss: 0.849816 [50816/60000]\n",
      "loss: 0.863956 [50880/60000]\n",
      "loss: 0.914083 [50944/60000]\n",
      "loss: 0.870574 [57408/60000]\n",
      "loss: 0.730923 [57472/60000]\n",
      "loss: 0.956163 [57536/60000]\n",
      "loss: 0.882592 [57600/60000]\n",
      "loss: 0.838338 [57920/60000]\n",
      "loss: 0.724704 [57984/60000]\n",
      "loss: 0.867263 [58048/60000]\n",
      "loss: 0.969479 [58112/60000]\n",
      "loss: 1.078968 [58432/60000]\n",
      "loss: 0.938262 [58496/60000]\n",
      "loss: 0.782533 [58560/60000]\n",
      "loss: 0.873522 [58624/60000]\n",
      "loss: 0.995015 [58944/60000]\n",
      "loss: 0.763581 [59008/60000]\n",
      "loss: 0.841061 [59072/60000]\n",
      "loss: 0.864526 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.899547 \n",
      "\n",
      "Epoch 8\n",
      "\n",
      "loss: 0.934701 [   64/60000]\n",
      "loss: 0.751509 [  128/60000]\n",
      "loss: 0.891600 [  192/60000]\n",
      "loss: 1.005820 [  256/60000]\n",
      "loss: 0.968988 [  576/60000]\n",
      "loss: 0.946810 [  640/60000]\n",
      "loss: 0.720799 [  704/60000]\n",
      "loss: 0.720246 [  768/60000]\n",
      "loss: 0.921616 [ 1088/60000]\n",
      "loss: 1.014274 [ 1152/60000]\n",
      "loss: 0.876516 [ 1216/60000]\n",
      "loss: 1.014013 [ 1280/60000]\n",
      "loss: 0.713442 [ 1600/60000]\n",
      "loss: 0.783157 [ 1664/60000]\n",
      "loss: 0.779104 [ 1728/60000]\n",
      "loss: 0.886499 [ 1792/60000]\n",
      "loss: 0.895272 [ 8256/60000]\n",
      "loss: 0.782516 [ 8320/60000]\n",
      "loss: 0.927559 [ 8384/60000]\n",
      "loss: 0.917768 [ 8448/60000]\n",
      "loss: 0.833950 [ 8768/60000]\n",
      "loss: 0.863602 [ 8832/60000]\n",
      "loss: 0.910634 [ 8896/60000]\n",
      "loss: 1.059880 [ 8960/60000]\n",
      "loss: 0.804186 [ 9280/60000]\n",
      "loss: 0.736549 [ 9344/60000]\n",
      "loss: 0.852708 [ 9408/60000]\n",
      "loss: 0.935131 [ 9472/60000]\n",
      "loss: 0.910000 [ 9792/60000]\n",
      "loss: 1.009429 [ 9856/60000]\n",
      "loss: 0.827844 [ 9920/60000]\n",
      "loss: 0.970903 [ 9984/60000]\n",
      "loss: 0.763408 [16448/60000]\n",
      "loss: 0.714424 [16512/60000]\n",
      "loss: 0.913742 [16576/60000]\n",
      "loss: 0.949552 [16640/60000]\n",
      "loss: 0.816243 [16960/60000]\n",
      "loss: 0.901178 [17024/60000]\n",
      "loss: 0.846183 [17088/60000]\n",
      "loss: 0.965389 [17152/60000]\n",
      "loss: 0.971936 [17472/60000]\n",
      "loss: 1.035015 [17536/60000]\n",
      "loss: 0.769083 [17600/60000]\n",
      "loss: 0.957420 [17664/60000]\n",
      "loss: 0.885917 [17984/60000]\n",
      "loss: 0.997973 [18048/60000]\n",
      "loss: 1.108441 [18112/60000]\n",
      "loss: 0.785597 [18176/60000]\n",
      "loss: 0.829277 [24640/60000]\n",
      "loss: 0.937668 [24704/60000]\n",
      "loss: 0.854722 [24768/60000]\n",
      "loss: 1.024924 [24832/60000]\n",
      "loss: 0.896692 [25152/60000]\n",
      "loss: 0.787660 [25216/60000]\n",
      "loss: 0.744682 [25280/60000]\n",
      "loss: 0.795130 [25344/60000]\n",
      "loss: 0.834537 [25664/60000]\n",
      "loss: 0.803049 [25728/60000]\n",
      "loss: 0.901644 [25792/60000]\n",
      "loss: 0.783404 [25856/60000]\n",
      "loss: 0.906857 [26176/60000]\n",
      "loss: 1.131196 [26240/60000]\n",
      "loss: 0.928405 [26304/60000]\n",
      "loss: 0.767094 [26368/60000]\n",
      "loss: 0.874888 [32832/60000]\n",
      "loss: 0.880445 [32896/60000]\n",
      "loss: 0.898530 [32960/60000]\n",
      "loss: 1.026028 [33024/60000]\n",
      "loss: 0.732264 [33344/60000]\n",
      "loss: 0.846740 [33408/60000]\n",
      "loss: 0.803006 [33472/60000]\n",
      "loss: 0.963064 [33536/60000]\n",
      "loss: 0.850807 [33856/60000]\n",
      "loss: 0.797467 [33920/60000]\n",
      "loss: 0.983225 [33984/60000]\n",
      "loss: 0.817520 [34048/60000]\n",
      "loss: 0.834173 [34368/60000]\n",
      "loss: 0.954759 [34432/60000]\n",
      "loss: 0.705477 [34496/60000]\n",
      "loss: 0.788093 [34560/60000]\n",
      "loss: 0.894952 [41024/60000]\n",
      "loss: 0.839694 [41088/60000]\n",
      "loss: 0.699091 [41152/60000]\n",
      "loss: 0.817603 [41216/60000]\n",
      "loss: 0.969655 [41536/60000]\n",
      "loss: 0.797618 [41600/60000]\n",
      "loss: 0.866973 [41664/60000]\n",
      "loss: 0.895545 [41728/60000]\n",
      "loss: 0.877438 [42048/60000]\n",
      "loss: 0.933922 [42112/60000]\n",
      "loss: 0.736303 [42176/60000]\n",
      "loss: 0.789703 [42240/60000]\n",
      "loss: 0.893026 [42560/60000]\n",
      "loss: 0.637709 [42624/60000]\n",
      "loss: 0.835574 [42688/60000]\n",
      "loss: 0.992872 [42752/60000]\n",
      "loss: 0.790311 [49216/60000]\n",
      "loss: 0.805845 [49280/60000]\n",
      "loss: 0.911897 [49344/60000]\n",
      "loss: 0.723603 [49408/60000]\n",
      "loss: 0.838921 [49728/60000]\n",
      "loss: 0.836173 [49792/60000]\n",
      "loss: 0.647143 [49856/60000]\n",
      "loss: 0.873812 [49920/60000]\n",
      "loss: 0.830418 [50240/60000]\n",
      "loss: 1.026358 [50304/60000]\n",
      "loss: 0.872835 [50368/60000]\n",
      "loss: 0.771211 [50432/60000]\n",
      "loss: 0.789905 [50752/60000]\n",
      "loss: 0.793099 [50816/60000]\n",
      "loss: 0.810614 [50880/60000]\n",
      "loss: 0.858083 [50944/60000]\n",
      "loss: 0.819404 [57408/60000]\n",
      "loss: 0.667837 [57472/60000]\n",
      "loss: 0.907404 [57536/60000]\n",
      "loss: 0.825681 [57600/60000]\n",
      "loss: 0.789200 [57920/60000]\n",
      "loss: 0.667585 [57984/60000]\n",
      "loss: 0.816477 [58048/60000]\n",
      "loss: 0.925454 [58112/60000]\n",
      "loss: 1.035748 [58432/60000]\n",
      "loss: 0.886666 [58496/60000]\n",
      "loss: 0.719870 [58560/60000]\n",
      "loss: 0.818864 [58624/60000]\n",
      "loss: 0.941456 [58944/60000]\n",
      "loss: 0.708827 [59008/60000]\n",
      "loss: 0.793524 [59072/60000]\n",
      "loss: 0.813695 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.849304 \n",
      "\n",
      "Epoch 9\n",
      "\n",
      "loss: 0.869879 [   64/60000]\n",
      "loss: 0.696418 [  128/60000]\n",
      "loss: 0.850374 [  192/60000]\n",
      "loss: 0.958181 [  256/60000]\n",
      "loss: 0.933248 [  576/60000]\n",
      "loss: 0.914612 [  640/60000]\n",
      "loss: 0.666190 [  704/60000]\n",
      "loss: 0.666128 [  768/60000]\n",
      "loss: 0.874919 [ 1088/60000]\n",
      "loss: 0.975714 [ 1152/60000]\n",
      "loss: 0.829787 [ 1216/60000]\n",
      "loss: 0.956389 [ 1280/60000]\n",
      "loss: 0.657824 [ 1600/60000]\n",
      "loss: 0.727485 [ 1664/60000]\n",
      "loss: 0.731919 [ 1728/60000]\n",
      "loss: 0.833772 [ 1792/60000]\n",
      "loss: 0.838899 [ 8256/60000]\n",
      "loss: 0.732002 [ 8320/60000]\n",
      "loss: 0.890392 [ 8384/60000]\n",
      "loss: 0.868963 [ 8448/60000]\n",
      "loss: 0.786622 [ 8768/60000]\n",
      "loss: 0.808650 [ 8832/60000]\n",
      "loss: 0.857910 [ 8896/60000]\n",
      "loss: 1.018485 [ 8960/60000]\n",
      "loss: 0.753722 [ 9280/60000]\n",
      "loss: 0.679161 [ 9344/60000]\n",
      "loss: 0.810013 [ 9408/60000]\n",
      "loss: 0.891927 [ 9472/60000]\n",
      "loss: 0.872860 [ 9792/60000]\n",
      "loss: 0.964333 [ 9856/60000]\n",
      "loss: 0.774975 [ 9920/60000]\n",
      "loss: 0.917323 [ 9984/60000]\n",
      "loss: 0.708949 [16448/60000]\n",
      "loss: 0.665695 [16512/60000]\n",
      "loss: 0.880555 [16576/60000]\n",
      "loss: 0.902758 [16640/60000]\n",
      "loss: 0.764453 [16960/60000]\n",
      "loss: 0.853031 [17024/60000]\n",
      "loss: 0.796425 [17088/60000]\n",
      "loss: 0.911049 [17152/60000]\n",
      "loss: 0.932446 [17472/60000]\n",
      "loss: 0.989800 [17536/60000]\n",
      "loss: 0.719180 [17600/60000]\n",
      "loss: 0.919148 [17664/60000]\n",
      "loss: 0.833340 [17984/60000]\n",
      "loss: 0.964791 [18048/60000]\n",
      "loss: 1.062789 [18112/60000]\n",
      "loss: 0.735249 [18176/60000]\n",
      "loss: 0.793011 [24640/60000]\n",
      "loss: 0.896436 [24704/60000]\n",
      "loss: 0.798251 [24768/60000]\n",
      "loss: 0.983379 [24832/60000]\n",
      "loss: 0.845115 [25152/60000]\n",
      "loss: 0.730157 [25216/60000]\n",
      "loss: 0.704677 [25280/60000]\n",
      "loss: 0.748548 [25344/60000]\n",
      "loss: 0.793421 [25664/60000]\n",
      "loss: 0.750004 [25728/60000]\n",
      "loss: 0.862406 [25792/60000]\n",
      "loss: 0.725170 [25856/60000]\n",
      "loss: 0.864349 [26176/60000]\n",
      "loss: 1.102102 [26240/60000]\n",
      "loss: 0.893828 [26304/60000]\n",
      "loss: 0.716971 [26368/60000]\n",
      "loss: 0.829837 [32832/60000]\n",
      "loss: 0.840799 [32896/60000]\n",
      "loss: 0.856490 [32960/60000]\n",
      "loss: 0.978410 [33024/60000]\n",
      "loss: 0.679661 [33344/60000]\n",
      "loss: 0.817203 [33408/60000]\n",
      "loss: 0.763545 [33472/60000]\n",
      "loss: 0.933565 [33536/60000]\n",
      "loss: 0.806945 [33856/60000]\n",
      "loss: 0.740894 [33920/60000]\n",
      "loss: 0.947544 [33984/60000]\n",
      "loss: 0.775933 [34048/60000]\n",
      "loss: 0.785550 [34368/60000]\n",
      "loss: 0.910168 [34432/60000]\n",
      "loss: 0.655190 [34496/60000]\n",
      "loss: 0.744181 [34560/60000]\n",
      "loss: 0.853546 [41024/60000]\n",
      "loss: 0.791881 [41088/60000]\n",
      "loss: 0.660522 [41152/60000]\n",
      "loss: 0.784266 [41216/60000]\n",
      "loss: 0.923606 [41536/60000]\n",
      "loss: 0.743590 [41600/60000]\n",
      "loss: 0.832828 [41664/60000]\n",
      "loss: 0.854683 [41728/60000]\n",
      "loss: 0.836642 [42048/60000]\n",
      "loss: 0.892733 [42112/60000]\n",
      "loss: 0.691496 [42176/60000]\n",
      "loss: 0.732721 [42240/60000]\n",
      "loss: 0.851267 [42560/60000]\n",
      "loss: 0.593548 [42624/60000]\n",
      "loss: 0.798850 [42688/60000]\n",
      "loss: 0.949344 [42752/60000]\n",
      "loss: 0.736170 [49216/60000]\n",
      "loss: 0.769543 [49280/60000]\n",
      "loss: 0.877151 [49344/60000]\n",
      "loss: 0.685214 [49408/60000]\n",
      "loss: 0.809716 [49728/60000]\n",
      "loss: 0.793532 [49792/60000]\n",
      "loss: 0.601495 [49856/60000]\n",
      "loss: 0.834400 [49920/60000]\n",
      "loss: 0.787684 [50240/60000]\n",
      "loss: 0.989242 [50304/60000]\n",
      "loss: 0.834180 [50368/60000]\n",
      "loss: 0.726067 [50432/60000]\n",
      "loss: 0.744658 [50752/60000]\n",
      "loss: 0.751324 [50816/60000]\n",
      "loss: 0.772482 [50880/60000]\n",
      "loss: 0.814224 [50944/60000]\n",
      "loss: 0.782078 [57408/60000]\n",
      "loss: 0.619996 [57472/60000]\n",
      "loss: 0.871719 [57536/60000]\n",
      "loss: 0.781831 [57600/60000]\n",
      "loss: 0.754589 [57920/60000]\n",
      "loss: 0.627350 [57984/60000]\n",
      "loss: 0.778452 [58048/60000]\n",
      "loss: 0.891115 [58112/60000]\n",
      "loss: 1.001571 [58432/60000]\n",
      "loss: 0.848183 [58496/60000]\n",
      "loss: 0.673686 [58560/60000]\n",
      "loss: 0.778396 [58624/60000]\n",
      "loss: 0.899565 [58944/60000]\n",
      "loss: 0.669119 [59008/60000]\n",
      "loss: 0.757421 [59072/60000]\n",
      "loss: 0.775383 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.811548 \n",
      "\n",
      "Epoch 10\n",
      "\n",
      "loss: 0.818380 [   64/60000]\n",
      "loss: 0.655083 [  128/60000]\n",
      "loss: 0.818375 [  192/60000]\n",
      "loss: 0.922535 [  256/60000]\n",
      "loss: 0.904382 [  576/60000]\n",
      "loss: 0.890724 [  640/60000]\n",
      "loss: 0.626821 [  704/60000]\n",
      "loss: 0.627468 [  768/60000]\n",
      "loss: 0.838778 [ 1088/60000]\n",
      "loss: 0.946598 [ 1152/60000]\n",
      "loss: 0.796317 [ 1216/60000]\n",
      "loss: 0.909344 [ 1280/60000]\n",
      "loss: 0.617232 [ 1600/60000]\n",
      "loss: 0.686175 [ 1664/60000]\n",
      "loss: 0.697646 [ 1728/60000]\n",
      "loss: 0.792082 [ 1792/60000]\n",
      "loss: 0.796410 [ 8256/60000]\n",
      "loss: 0.693750 [ 8320/60000]\n",
      "loss: 0.864867 [ 8384/60000]\n",
      "loss: 0.832594 [ 8448/60000]\n",
      "loss: 0.750549 [ 8768/60000]\n",
      "loss: 0.767163 [ 8832/60000]\n",
      "loss: 0.818657 [ 8896/60000]\n",
      "loss: 0.986080 [ 8960/60000]\n",
      "loss: 0.713770 [ 9280/60000]\n",
      "loss: 0.636895 [ 9344/60000]\n",
      "loss: 0.777219 [ 9408/60000]\n",
      "loss: 0.859176 [ 9472/60000]\n",
      "loss: 0.847573 [ 9792/60000]\n",
      "loss: 0.929838 [ 9856/60000]\n",
      "loss: 0.734542 [ 9920/60000]\n",
      "loss: 0.874235 [ 9984/60000]\n",
      "loss: 0.668790 [16448/60000]\n",
      "loss: 0.629343 [16512/60000]\n",
      "loss: 0.855837 [16576/60000]\n",
      "loss: 0.865274 [16640/60000]\n",
      "loss: 0.725190 [16960/60000]\n",
      "loss: 0.814888 [17024/60000]\n",
      "loss: 0.759648 [17088/60000]\n",
      "loss: 0.868142 [17152/60000]\n",
      "loss: 0.900965 [17472/60000]\n",
      "loss: 0.952914 [17536/60000]\n",
      "loss: 0.683263 [17600/60000]\n",
      "loss: 0.891465 [17664/60000]\n",
      "loss: 0.792918 [17984/60000]\n",
      "loss: 0.939524 [18048/60000]\n",
      "loss: 1.025474 [18112/60000]\n",
      "loss: 0.698004 [18176/60000]\n",
      "loss: 0.765759 [24640/60000]\n",
      "loss: 0.863777 [24704/60000]\n",
      "loss: 0.755214 [24768/60000]\n",
      "loss: 0.949933 [24832/60000]\n",
      "loss: 0.805133 [25152/60000]\n",
      "loss: 0.686234 [25216/60000]\n",
      "loss: 0.674086 [25280/60000]\n",
      "loss: 0.710962 [25344/60000]\n",
      "loss: 0.762699 [25664/60000]\n",
      "loss: 0.707129 [25728/60000]\n",
      "loss: 0.833914 [25792/60000]\n",
      "loss: 0.680027 [25856/60000]\n",
      "loss: 0.830781 [26176/60000]\n",
      "loss: 1.079837 [26240/60000]\n",
      "loss: 0.866187 [26304/60000]\n",
      "loss: 0.677052 [26368/60000]\n",
      "loss: 0.795066 [32832/60000]\n",
      "loss: 0.809433 [32896/60000]\n",
      "loss: 0.823820 [32960/60000]\n",
      "loss: 0.935789 [33024/60000]\n",
      "loss: 0.638763 [33344/60000]\n",
      "loss: 0.795603 [33408/60000]\n",
      "loss: 0.735044 [33472/60000]\n",
      "loss: 0.910879 [33536/60000]\n",
      "loss: 0.773511 [33856/60000]\n",
      "loss: 0.695023 [33920/60000]\n",
      "loss: 0.919802 [33984/60000]\n",
      "loss: 0.744751 [34048/60000]\n",
      "loss: 0.746475 [34368/60000]\n",
      "loss: 0.874463 [34432/60000]\n",
      "loss: 0.619095 [34496/60000]\n",
      "loss: 0.707794 [34560/60000]\n",
      "loss: 0.822714 [41024/60000]\n",
      "loss: 0.755281 [41088/60000]\n",
      "loss: 0.632151 [41152/60000]\n",
      "loss: 0.759733 [41216/60000]\n",
      "loss: 0.886887 [41536/60000]\n",
      "loss: 0.699767 [41600/60000]\n",
      "loss: 0.808232 [41664/60000]\n",
      "loss: 0.823164 [41728/60000]\n",
      "loss: 0.804463 [42048/60000]\n",
      "loss: 0.859308 [42112/60000]\n",
      "loss: 0.656687 [42176/60000]\n",
      "loss: 0.688568 [42240/60000]\n",
      "loss: 0.819222 [42560/60000]\n",
      "loss: 0.560635 [42624/60000]\n",
      "loss: 0.769502 [42688/60000]\n",
      "loss: 0.912398 [42752/60000]\n",
      "loss: 0.693504 [49216/60000]\n",
      "loss: 0.741858 [49280/60000]\n",
      "loss: 0.849360 [49344/60000]\n",
      "loss: 0.655613 [49408/60000]\n",
      "loss: 0.788571 [49728/60000]\n",
      "loss: 0.760336 [49792/60000]\n",
      "loss: 0.566534 [49856/60000]\n",
      "loss: 0.803936 [49920/60000]\n",
      "loss: 0.753757 [50240/60000]\n",
      "loss: 0.958666 [50304/60000]\n",
      "loss: 0.803418 [50368/60000]\n",
      "loss: 0.690076 [50432/60000]\n",
      "loss: 0.709773 [50752/60000]\n",
      "loss: 0.718982 [50816/60000]\n",
      "loss: 0.743456 [50880/60000]\n",
      "loss: 0.777803 [50944/60000]\n",
      "loss: 0.753205 [57408/60000]\n",
      "loss: 0.581966 [57472/60000]\n",
      "loss: 0.843700 [57536/60000]\n",
      "loss: 0.746454 [57600/60000]\n",
      "loss: 0.729056 [57920/60000]\n",
      "loss: 0.597840 [57984/60000]\n",
      "loss: 0.748410 [58048/60000]\n",
      "loss: 0.862348 [58112/60000]\n",
      "loss: 0.973030 [58432/60000]\n",
      "loss: 0.817569 [58496/60000]\n",
      "loss: 0.638156 [58560/60000]\n",
      "loss: 0.746774 [58624/60000]\n",
      "loss: 0.865035 [58944/60000]\n",
      "loss: 0.638671 [59008/60000]\n",
      "loss: 0.727644 [59072/60000]\n",
      "loss: 0.744400 [59136/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.781506 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lerning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
